\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{url}
\usepackage{color}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\textheight=8.85in
\usepackage{matlab-prettifier}
\pagestyle{myheadings}
\usepackage{subcaption}

\setlength{\tabcolsep}{0in}
\begin{document}


\thispagestyle {empty}

\newcommand{\lsp}[1]{\large\renewcommand{\baselinestretch}{#1}\normalsize}
\newcommand{\hsp}{\hspace{.2in}}
\newcommand{\comment}[1]{}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{problem}{Problem}[section]

\newcommand{\R}{{\rm\hbox{I\kern-.15em R}}}
\newcommand{\IR}{{\rm\hbox{I\kern-.15em R}}}
\newcommand{\IN}{{\rm\hbox{I\kern-.15em N}}}
\newcommand{\IZ}{{\sf\hbox{Z\kern-.40em Z}}}
\newcommand{\IS}{{\rm\hbox{S\kern-.45em S}}}
\newcommand{\Real}{I\!\!R}


\newcommand{\linesep}{\vspace{.2cm}\hrule\vspace{0.2cm}}
\newcommand{\categorysep}{\vspace{0.5cm}}
\newcommand{\entrysep}{\vspace{0cm}}

\newcommand{\category}[1]{\categorysep
                  \noindent {\bf \large #1}
              \linesep}

\pagestyle{empty}

\begin{center}
{\large \textbf{CSE 847 (Spring 2023): Machine Learning--- Homework 5\\
 Instructor: Jiayu Zhou \\
  Balija Santoshkumar}} \\
\href{mailto:balijasa@msu.edu}{balijasa@msu.edu}

\href{https://github.com/santoshbalija/CSE847_HW5}{https://github.com/santoshbalija/CSE847\_HW5}
\end{center}
\section{ Clustering: K-means}

\begin{enumerate}
\item  Elaborate the relationship between $k$-means and spectral relaxation of $k$-means. Is it possible that we obtain exact $k$-means solution using spectral relaxed $k$-means?
\item Implementation of $k$-means. Submit all the source code to D2L along with a short report on your observation.
\begin{itemize}
	\item Implement the $k$-means in MATLAB using the alternating procedure introduced in the class (you will not get the credit if you use the build-in kmeans function in MATLAB).
\item  Implement the spectral relaxation of $k$-means. Create a random dataset and compare the $k$-means and spectral relaxed $k$-means.
\end{itemize}
\end{enumerate}

\textbf{Sol:}

We assume that we have $n$ data points $\left\{x_i\right\}_{i=1}^n \in \mathbb{R}^m$, which we organize as columns in a matrix
$$
X=\left[x_1, x_2, \cdots, x_n\right] \in \mathbb{R}^{m \times n}
$$


The objective of K-means is reduce sum squared error(SSE)

\begin{equation}
	q_j=\sum_{v \in \pi_j}\left\|x_v-c_j\right\|^2
\end{equation}
where $c_j$ is the cluster center of the corresponding cluster
Let $e$ be the vector of all ones with appropriate length. It is easy to see that $c_j=X_j e / n_j$, where $X_j$ is the data matrix of the $j$-th cluster.

SSE can transformed into
\begin{equation}
	q_j=\sum_{j=1}^k\left(\operatorname{trace}\left(X_j^T X_j\right)-\frac{e^T}{\sqrt{n_j}} X_j^T X_j \frac{e}{\sqrt{n_j}}\right)
\end{equation}

Define the $n$-by- $k$ orthogonal matrix $Y$ as follows
$$
Y=\left(\begin{array}{cccc}
	e / \sqrt{n_1} & & \\
	& e / \sqrt{n_2} & \\
	& & \vdots & \\
	& & & e / \sqrt{n_k}
\end{array}\right)
$$
Then
$$
Q(\Pi)=\operatorname{trace}\left(X^T X\right)-\operatorname{trace}\left(Y^T X^T X Y\right) .
$$
The $k$-means objective, minimization of $Q(\Pi)$, is equivalent to the maximization of trace $\left(Y^T X^T X Y\right)$ with $Y$ .

In  spectral
relaxation of k-means is instead of using this specific expression for Y , it is
possible to use any arbitrary orthogonal matrix for Y . This leads to the relaxed maximization problem
\begin{equation}
	\max _{Y^T Y=I_k} \operatorname{trace}\left(Y^T X^T X Y\right)
\end{equation}
The first k vectors in the left singular matrix of X can produce the Y   that maximizes this expression in Eq.3
we can see that both k-means and spectral k-means are trying to minimize the same error function, but spectral k-means is  first trying to project the dataset into a lower dimensional space which makes it easier to capture the complex clustering structures.

The spectral-relaxed k-means become completely equivalent to k-means when the expression for Y becomes
equal to the matrix mentioned in Eq. 2

\\

I have used fisheriris data set for K-means

\\
\textbf{Regular K-means}

After implementation we see SSE as

\begin{verbatim}
SSE for k=3: 26.893170
SSE for k=4: 26.067859
SSE for k=5: 14.691709
SSE for k=6: 11.985356
SSE for k=7: 8.231259
SSE for k=8: 6.774484
SSE for k=9: 6.016505
SSE for k=10: 5.094189
\end{verbatim} 


\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_3.png}
		\caption{No of Clusters 3}
		\label{fig:cluster3}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_4.png}
		\caption{No of Clusters 4}
		\label{fig:cluster4}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_5.png}
		\caption{No of Clusters 5}
		\label{fig:cluster5}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_6.png}
		\caption{No of Clusters 6}
		\label{fig:cluster6}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_7.png}
		\caption{No of Clusters7}
		\label{fig:cluster7}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_8.png}
		\caption{No of Clusters 8}
		\label{fig:cluster8}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_9.png}
		\caption{No of Clusters 9}
		\label{fig:cluster9}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_10.png}
		\caption{No of Clusters 10}
		\label{fig:cluster10}
	\end{subfigure}
	\caption{Cluster assignments with K-means}
	\label{fig:clusterassignments}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{./Results/KMeans/SSE_Convergence.png}
	\caption{Convergence of SSE with Clusters variation in KMeans}
	\label{fig:SSEconvergence}
\end{figure}

\textbf{ spectral relaxation of $k$-means}


After implementation we see Final SSE as

\begin{verbatim}
SSE for k=3: 145.287135
SSE for k=4: 107.839511
SSE for k=5: 127.624815
SSE for k=6: 82.031575
SSE for k=7: 106.387628
SSE for k=8: 70.359438
SSE for k=9: 69.009363
SSE for k=10: 43.252894
\end{verbatim} 

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_S3.png}
		\caption{No of Clusters 3}
		\label{fig:cluster3}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_S4.png}
		\caption{No of Clusters 4}
		\label{fig:cluster4}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_S5.png}
		\caption{No of Clusters 5}
		\label{fig:cluster5}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_S6.png}
		\caption{No of Clusters 6}
		\label{fig:cluster6}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_S7.png}
		\caption{No of Clusters7}
		\label{fig:cluster7}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_S8.png}
		\caption{No of Clusters 8}
		\label{fig:cluster8}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_S9.png}
		\caption{No of Clusters 9}
		\label{fig:cluster9}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\includegraphics[width=\textwidth]{./Results/KMeans/Cluster_Arrangement_S10.png}
		\caption{No of Clusters 10}
		\label{fig:cluster10}
	\end{subfigure}
	\caption{Cluster assignments with Spectral relaxation K-means}
	\label{fig:clusterassignments}
\end{figure}
\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{./Results/KMeans/SSE_Convergence_S.png}
	\caption{Convergence of SSE with Clusters variation in Spectral relaxation KMeans}
	\label{fig:SSEconvergence}
\end{figure}

\clearpage

\section{Principle Component Analysis}

\begin{enumerate}
	\item  Suppose we have the following data points in $2 \mathrm{~d}$ space $(0,0),(-1,2),(-3,6),(1,-2),(3,-6)$.
	\begin{itemize}
\item Draw them on a 2-d plot, each data point being a dot.
\item  What is the first principle component? Given 1-2 sentences justification. You do not need to run MATLAB to get the answer.
\item  What is the second principle component? Given 1-2 sentences justification. You do not need to run MATLAB to get the answer.
\end{itemize}

\item  Experiment: We apply data pre-processing techniques to a collection of handwritten digit images from the USPS dataset (data in MATLAB format: USPS.mat) ${ }^1$. You can load the whole dataset into MATLAB by load USPS.mat. The matrix $A$ contains all the images of size 16 by 16. Each of the 3000 rows in $A$ corresponds to the image of one handwritten digit (between 0 and 9). To visualize a particular image, such as the second one, first you need to convert the vector representation of the image to the matrix representation by $A 2=$ reshape $(A(2,:), 16,16)$, and then use imshow $\left(A 2^{\prime}\right)$ for visualization.

Implement Principal Component Analysis (PCA) using SVD and apply to the data using $p=10,50,100,200$ principal components. Reconstruct images using the selected principal components from part 1.

\begin{itemize}
\item Show the source code links for parts 1 and 2 to your github account.
\item  The total reconstruction error for $p=10,50,100,200$.
\item  A subset (the first two) of the reconstructed images for $p=10,50,100,200$.
Note: The USPS dataset is available at http://www. csie.ntu.edu.tw/ cjlin/libsvmtools/ datasets/multiclass.html\#usps. The image size is 16 by 16 , thus the data dimensionality of the original dataset is 256 . We used a subset of 3000 images in this homework.
\end{itemize}
\end{enumerate}

\textbf{Sol:}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{./Results/PCA/plotintial.png}
	\caption{Data plot}
	\label{fig:plot_initial}
\end{figure}

By looking plot itself; we can identify first and second principal components. First component is along the data axis and second one is perpendicular to the first one. 

After implementation we see reconstruction error as

\begin{verbatim}
The reconstruction error for p=10 is: 672.504293
The reconstruction error for p=50 is: 581.322126
The reconstruction error for p=100 is: 557.849767
The reconstruction error for p=200 is: 546.151961
\end{verbatim} 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{./Results/PCA/plot_118.png}
	\caption{Visualialization of oroginal and reconstructed images  PCA with different P values.}
	\label{fig:plot_initial}
\end{figure}

\end{document}
